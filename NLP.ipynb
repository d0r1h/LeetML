{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "62cbaf15",
   "metadata": {},
   "source": [
    "1. [Calculate Unigram Probability from Corpus](#calculate-unigram-probability-from-corpus)\n",
    "2. [Calculate Perplexity for Language Models](#calculate-perplexity-for-language-models)\n",
    "3. [Exact Match Score with Normalization](#exact-match-score-with-normalization)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f898daf",
   "metadata": {},
   "source": [
    "### Calculate Unigram Probability from Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e135046e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1818"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = \"<s> Jack I like </s> <s> Jack I do like </s>\"\n",
    "word = \"Jack\"\n",
    "\n",
    "def unigram_probability(corpus: str, word: str) -> float:\n",
    "    \n",
    "    split_corpus = corpus.split(' ')\n",
    "    \n",
    "    count = sum([1 for i in corpus.split(' ') if i == word ])\n",
    "\n",
    "    return round(count/len(split_corpus) ,4)\n",
    "\n",
    "unigram_probability(corpus, word)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e647cea8",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4236cca4",
   "metadata": {},
   "source": [
    "### Calculate Perplexity for Language Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd17a235",
   "metadata": {},
   "source": [
    "Perplexity is mainly used to evaluate language models.\n",
    "\n",
    "For a sequence of tokens  w1,w2,…,wn :\n",
    "\n",
    "\n",
    "$$\n",
    "\\text{Perplexity} =\n",
    "\\exp\\left(\n",
    "-\\frac{1}{N}\n",
    "\\sum_{i=1}^{N}\n",
    "\\log P(w_i \\mid w_1, \\dots, w_{i-1})\n",
    "\\right)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b68a911",
   "metadata": {},
   "source": [
    "__Intuition__\n",
    "\n",
    "\n",
    "Perplexity answers, “On average, how many choices is the model confused between at each step?”\n",
    "\n",
    "- Perplexity = 1 → perfect prediction\n",
    "- Perplexity = 10 → model is choosing among ~10 words\n",
    "- Perplexity = 50 → very uncertain\n",
    "\n",
    "\n",
    "Suppose a sentence of 3 tokens:\n",
    "\n",
    "| Token | Probability |\n",
    "|-------|-------------|\n",
    "| $w_1$ | $P(w_1)=0.5$ |\n",
    "| $w_2$ | $P(w_2)=0.25$ |\n",
    "| $w_3$ | $P(w_3)=0.125$ |\n",
    "\n",
    "\n",
    "$$\n",
    "\\text{PPL} =\n",
    "\\exp\\left(\n",
    "-\\frac{1}{3}\n",
    "(\\log 0.5 + \\log 0.25 + \\log 0.125)\n",
    "\\right)\n",
    "$$\n",
    "\n",
    "\n",
    "$$\n",
    "\\boxed{\\text{Perplexity} = 4}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "20fff63e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "'''\n",
    "Function to calculate the perplexity of a language model given a sequence of token probabilities. where each \n",
    "probability represents the model's predicted probability for the actual next token in a sequence\n",
    "It quantifies how well a probability distribution predicts a sample - a lower perplexity indicates the model \n",
    "assigns higher probabilities to the actual observed tokens, meaning it's a better predictor.\n",
    "'''\n",
    "\n",
    "probabilities = [0.5, 0.5, 0.5, 0.5]\n",
    "\n",
    "def calculate_perplexity(probabilities: list[float]) -> float:\n",
    "\n",
    "    return np.exp(-(np.mean(np.log(probabilities))))\n",
    "\n",
    "\n",
    "calculate_perplexity(probabilities)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "500942c5",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d78a2fe",
   "metadata": {},
   "source": [
    "### Exact Match Score with Normalization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b9ac80e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "def exact_match_score(predictions: list[str], references: list[str]) -> float:\n",
    "    \n",
    "    if len(predictions) > 0:\n",
    "\n",
    "        cleaned_predictions = []\n",
    "        cleaned_references = []\n",
    "\n",
    "        for i in predictions:\n",
    "            i = i.lower()\n",
    "            clean = re.sub(r'[^\\w\\s]', '', i)\n",
    "            clean = re.sub(r'\\s+', ' ', clean).strip()\n",
    "            cleaned_predictions.append(clean)\n",
    "\n",
    "        for i in references:\n",
    "            i = i.lower()\n",
    "            clean = re.sub(r'[^\\w\\s]', '', i)\n",
    "            clean = re.sub(r'\\s+', ' ', clean).strip()\n",
    "            cleaned_references.append(clean)\n",
    "\n",
    "        true_pred = sum([i == j for i, j in zip(cleaned_predictions,cleaned_references)])\n",
    "        \n",
    "        \n",
    "        return round((true_pred/len(cleaned_predictions)),4)\n",
    "\n",
    "    else:\n",
    "        return 0.0\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e7bf9e9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = ['Hello, World!', 'The answer is 42']\n",
    "references = ['hello world', 'the answer is 42']\n",
    "\n",
    "exact_match_score(predictions, references)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb6b9e71",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ae6f5e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b6bdd1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dfcbc22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8811ec9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8cc2eeb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
